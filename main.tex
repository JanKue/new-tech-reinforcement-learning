\documentclass[runningheads]{llncs}

\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
\usepackage{hyperref}
\renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{New Tech in Reinforcement Learning}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Jan Küblbeck}
%
\authorrunning{J. Küblbeck}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Abstract
\end{abstract}
%
%
%
\section{Introduction}

an overview of different novel technology in the field of reinforcement learning

\section{Distributional Reinforcement Learning}

Ordinarily, the Bellman equation only considers the expected return of a policy. Two policies with the same expected return are considered equivalent, even if the underlying probability distributions are different and can lead to different results. Information is lost.

Distributional RL seeks to rectify this problem by considering the value distribution $Z$.\cite{bellemare2017distributional}

\section{Asynchronous Deep Reinforcement Learning}

Goals: require less specialized hardware, better performance than DQN/other deep RL

Multiple learners run in parallel, on different states using different policies.

\cite{mnih2016asynchronous} presents 4 algorithms adapted to asynchronous framework

\section{Hindsight Experience Replay}

HER is employed on top of deep RL algorithms such as DQN orr DDPG. HER allows learning multi-goal tasks from sparse and binary rewards.\cite{andrychowicz2017hindsight}

\section{Hierarchical Actor-Critic}

\cite{levy2017hierarchical} introduces a new technique for hierarchical reinforcement learning. Tasks are divided into subgoals.

\section{Conclusion}

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
% \begin{thebibliography}{8}
% \end{thebibliography}

\bibliographystyle{splncs04}
\bibliography{papers}

\end{document}