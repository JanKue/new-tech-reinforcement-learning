\documentclass[runningheads]{llncs}

\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
\usepackage{hyperref}
\renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{New Tech in Reinforcement Learning}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Jan Küblbeck}
%
\authorrunning{J. Küblbeck}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{KIT}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Abstract
\end{abstract}
%
%
%
\section{Introduction}

Reinforcement learning (RL), as an approach to machine learning, has been studied for decades. Especially the sub-field of deep reinforcement learning has attracted much attention during the last ten years, following the introduction of deep Q-learning \cite{mnih2013playing}, but new developments are not limited to this. This literature review provides an overview of several different novel technologies in the field of RL.

\section{Fundamentals of Reinforcement Learning}

\section{Domains}

\subsection{Distributional Reinforcement Learning}

Ordinarily, the Bellman equation only considers the expected return of a policy. Two policies with the same expected return are considered equivalent, even if the underlying probability distributions are different and can lead to different results. Information is lost.

Distributional RL seeks to rectify this problem by considering the value distribution $Z$.\cite{bellemare2017distributional}

\subsection{Asynchronous Deep Reinforcement Learning}

Goals: require less specialized hardware, better performance than DQN/other deep RL

Multiple learners run in parallel, on different states using different policies.

\cite{mnih2016asynchronous} presents 4 algorithms adapted to asynchronous framework

\subsection{Hindsight Experience Replay}

HER is employed on top of deep RL algorithms such as DQN or DDPG. HER allows learning multi-goal tasks from sparse and binary rewards.\cite{andrychowicz2017hindsight}

\subsection{Hierarchical Actor-Critic}

\cite{levy2017hierarchical} introduces a new technique for hierarchical reinforcement learning. Tasks are divided into subgoals.

\subsection{Feudal Networks}

\cite{vezhnevets2017feudal}

\section{Conclusion}

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
% \begin{thebibliography}{8}
% \end{thebibliography}

\bibliographystyle{splncs04}
\bibliography{papers}

\end{document}