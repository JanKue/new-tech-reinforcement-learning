\documentclass[runningheads]{llncs}

\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
\usepackage{hyperref}
\renewcommand\UrlFont{\color{blue}\rmfamily}
\usepackage{amsfonts}
\usepackage{amsmath}

\begin{document}
%
\title{New Tech in Reinforcement Learning}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Jan Küblbeck}
%
\authorrunning{J. Küblbeck}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{KIT}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Abstract
\end{abstract}
%
%
%
\section{Introduction}

Reinforcement learning (RL), as an approach to machine learning, has been studied for decades. Especially the sub-field of deep reinforcement learning has attracted much attention during the last ten years, following the introduction of deep Q-learning \cite{mnih2013playing}, but new developments are not limited to this. This literature review provides an overview of several different novel technologies in the field of RL.

\section{Fundamentals of Reinforcement Learning}

The standard framework of reinforcement learning is made up of an environment and an agent following a Markov decision process. The environment consists of the state space $\mathcal{S}$ and action space $\mathcal{A}$. Each state-action pair has an associated reward $R(s,a)$ and transition probabilities $P(s'|s,a)$. The Markov decision process also makes use of the discount factor $\gamma \in [0,1]$. In each state, the agent uses a policy $\pi$ to select its next action. \cite{sutton2018reinforcement}

\subsection{Deep Reinforcement Learning}

\section{Domains}

\subsection{Distributional Reinforcement Learning}

Ordinarily, the Bellman equation only considers the expected return of a policy. Two policies with the same expected return are considered equivalent, even if the underlying probability distributions are different and can lead to different results. Information is lost.

Distributional RL seeks to rectify this problem by considering the value distribution $Z$.\cite{bellemare2017distributional}

\subsection{Asynchronous Deep Reinforcement Learning}

Goals: require less specialized hardware, better performance than Deep Q-Networks (DQN)/other deep RL

Multiple learners run in parallel, on different states using different policies.

\cite{mnih2016asynchronous} presents 4 algorithms adapted to asynchronous framework

\subsection{Hindsight Experience Replay}

Ordinarily, solving complex problems with reinforcement learning requires a complicated reward function. The reward must be engineered to reflect the particular problem and produce the best possible outcome. The challenges of producing such an optimized reward function could be avoided by using sparse and binary rewards, e.g. $-1$ for not reaching a goal and $0$ for reaching it.

Hindsight Experience Replay (HER) \cite{andrychowicz2017hindsight} is an approach that allows learning from such sparse and binary rewards with a multi-goal framework. The technique is used on top of existing deep RL algorithms such as DQN or Deep Deterministic Policy Gradient (DDPG).

\subsection{Hierarchical Actor-Critic}

\cite{levy2017hierarchical} introduces a new technique for hierarchical reinforcement learning. Tasks are divided into subgoals.

\subsection{Feudal Networks}

\cite{vezhnevets2017feudal}

\section{Conclusion}

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
% \begin{thebibliography}{8}
% \end{thebibliography}

\bibliographystyle{splncs04}
\bibliography{bibliography}

\end{document}